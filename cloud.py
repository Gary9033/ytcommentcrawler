import streamlit as st
from googleapiclient.discovery import build
import pandas as pd
from wordcloud import WordCloud, STOPWORDS
import plotly.express as px
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import jieba
from collections import Counter
import re
from io import BytesIO

# âœ… å­—å‹è¨­å®šå¿…é ˆæ”¾æœ€é ‚ç«¯
FONT_PATH = "C:/Windows/Fonts/msjh.ttc"
fm.fontManager.addfont(FONT_PATH)
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# --- è¨­å®šå€ ---
API_KEY = 'AIzaSyCEzWvL9Zg6HXcPnC94JM_yA6ueP0trFWY'


def get_video_id(url):
    if 'shorts/' in url:
        return url.split('shorts/')[1].split('?')[0]
    elif 'v=' in url:
        return url.split('v=')[1].split('&')[0]
    elif 'youtu.be/' in url:
        return url.split('/')[-1].split('?')[0]
    return url


def get_video_comments(video_id):
    youtube = build('youtube', 'v3', developerKey=API_KEY)
    all_data = []
    nextPageToken = None
    status_placeholder = st.empty()

    try:
        while True:
            request = youtube.commentThreads().list(
                part="snippet,replies",
                videoId=video_id,
                maxResults=100,
                pageToken=nextPageToken,
                textFormat="plainText"
            )
            response = request.execute()

            for item in response['items']:
                top = item['snippet']['topLevelComment']['snippet']
                all_data.append({
                    "é¡å‹": "é ‚å±¤ç•™è¨€",
                    "ä½¿ç”¨è€…": top['authorDisplayName'],
                    "å…§å®¹": top['textDisplay'],
                    "é»è®š": top['likeCount'],
                    "æ™‚é–“": top['publishedAt']
                })

                if 'replies' in item:
                    for reply in item['replies']['comments']:
                        rep = reply['snippet']
                        all_data.append({
                            "é¡å‹": "â”” ç•™è¨€å›è¦†",
                            "ä½¿ç”¨è€…": rep['authorDisplayName'],
                            "å…§å®¹": rep['textDisplay'],
                            "é»è®š": rep['likeCount'],
                            "æ™‚é–“": rep['publishedAt']
                        })

            status_placeholder.info(f"ğŸš€ å·²æŠ“å– {len(all_data)} ç­†è³‡æ–™...")
            nextPageToken = response.get('nextPageToken')
            if not nextPageToken:
                break

        status_placeholder.empty()
        return all_data
    except Exception as e:
        st.error(f"ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None


def generate_wordcloud(df):
    # â”€â”€ æ–‡å­—æ¸…ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    all_text = " ".join(df["å…§å®¹"].astype(str).tolist())
    all_text = re.sub(r'http\S+|www\S+', '', all_text)
    all_text = re.sub(r'[^\w\s\u3040-\u30ff\u4e00-\u9fff]', ' ', all_text)

    # â”€â”€ Stopwords â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    custom_stopwords = set(STOPWORDS)
    custom_stopwords.update([
        "çš„", "äº†", "æ˜¯", "æˆ‘", "ä½ ", "ä»–", "å¥¹", "å®ƒ", "å€‘",
        "åœ¨", "ä¹Ÿ", "éƒ½", "å°±", "å’Œ", "æœ‰", "ä¸", "é€™", "é‚£",
        "ä¸€", "å—", "å•Š", "å§", "å“¦", "å“ˆ", "å¥½", "èªª", "ä¾†",
        "å»", "æœƒ", "è¦", "å¯ä»¥", "å› ç‚º", "æ‰€ä»¥", "ä½†æ˜¯", "é‚„æ˜¯",
        "å¦‚æœ", "é€™å€‹", "é‚£å€‹", "ä»€éº¼", "æ€éº¼", "ç‚ºä»€éº¼", "Reply",
        "replies", "reply", "https", "www", "com", "the", "to",
        "and", "of", "is", "it", "in", "that", "this"
    ])

    # â”€â”€ jieba åˆ†è©ï¼ˆåªåšä¸€æ¬¡ï¼ŒWordCloud å’Œ Bar Chart å…±ç”¨ï¼‰â”€â”€
    words = list(jieba.cut(all_text, cut_all=False))
    words_filtered = [w.strip() for w in words if len(w.strip()) > 1]
    segmented = " ".join(words_filtered)

    # â”€â”€ ç”¢ç”Ÿ WordCloudï¼ˆé«˜è§£æåº¦ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    wc = WordCloud(
        font_path=FONT_PATH,
        width=2400,               # âœ… é«˜è§£æåº¦ç•«å¸ƒ
        height=1200,
        scale=2,                  # âœ… æ–‡å­—é‚Šç·£æ›´å¹³æ»‘
        background_color="white",
        max_words=200,
        stopwords=custom_stopwords,
        collocations=False,
        prefer_horizontal=0.7,
        min_font_size=10,
        max_font_size=160,
        colormap="tab20",
        random_state=42,
        regexp=r"[\w\u3040-\u30ff\u4e00-\u9fff]+"
    ).generate(segmented)

    # â”€â”€ é¡¯ç¤º Word Cloud â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.subheader("â˜ï¸ ç•™è¨€ Word Cloud")
    fig, ax = plt.subplots(figsize=(16, 8), dpi=150)   # âœ… é«˜ DPI
    ax.imshow(wc, interpolation="bilinear")
    ax.axis("off")
    plt.tight_layout(pad=0)
    st.pyplot(fig)

    # âœ… ä¸‹è¼‰é«˜è§£æåº¦ PNGï¼ˆ300 DPIï¼‰
    buf = BytesIO()
    fig.savefig(buf, format="png", dpi=300, bbox_inches='tight')
    buf.seek(0)
    st.download_button(
        label="ğŸ“¥ ä¸‹è¼‰é«˜è§£æåº¦ Word Cloud PNG",
        data=buf,
        file_name="wordcloud_HQ.png",
        mime="image/png"
    )
    plt.close(fig)

    # â”€â”€ Top 20 é«˜é »è©ï¼ˆçœŸå¯¦å‡ºç¾æ¬¡æ•¸ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.subheader("ğŸ“Š Top 20 é«˜é »è©")

    words_for_count = [
        w for w in words_filtered
        if w not in custom_stopwords
    ]
    word_count = Counter(words_for_count)
    top20 = word_count.most_common(20)

    if top20:
        chart_df = pd.DataFrame(top20, columns=["è©èª", "å‡ºç¾æ¬¡æ•¸"])

        st.dataframe(
            chart_df.style.bar(
                subset=["å‡ºç¾æ¬¡æ•¸"],   # âœ… åªåœ¨ã€Œå‡ºç¾æ¬¡æ•¸ã€æ¬„é¡¯ç¤ºé•·æ¢
                color="#4c9be8",       # âœ… é•·æ¢é¡è‰²
                vmin=0
            ),
            width='stretch',
            hide_index=True            # âœ… éš±è—å·¦å´ index æ•¸å­—
        )
# --- åˆå§‹åŒ– Session State ---
if 'running' not in st.session_state:
    st.session_state.running = False
if 'result_df' not in st.session_state:
    st.session_state.result_df = None
if 'result_video_id' not in st.session_state:
    st.session_state.result_video_id = None


def start_running():
    st.session_state.running = True
    st.session_state.result_df = None


# --- Streamlit ç¶²é ä»‹é¢ ---
st.set_page_config(page_title="YouTube ç•™è¨€æŠ“å–å™¨", page_icon="ğŸ“Š", layout="wide")
st.title("ğŸ¥ YouTube ç•™è¨€å…¨éƒ¨æŠ“å–å·¥å…·")
st.write("è¼¸å…¥å½±ç‰‡ç¶²å€ï¼Œè‡ªå‹•æŠ“å–**æ‰€æœ‰**ç•™è¨€ï¼ˆåŒ…å«å›è¦†ï¼‰ä¸¦åŒ¯å‡ºã€‚")

url_input = st.text_input(
    "è«‹è²¼ä¸Š YouTube ç¶²å€:",
    placeholder="https://www.youtube.com/watch?v=...",
    disabled=st.session_state.running
)

st.button("é–‹å§‹åŸ·è¡ŒæŠ“å–", disabled=st.session_state.running, on_click=start_running)

# --- åŸ·è¡Œé‚è¼¯ ---
if st.session_state.running:
    if not url_input:
        st.warning("è«‹å…ˆè¼¸å…¥ç¶²å€ï¼")
        st.session_state.running = False
    else:
        v_id = get_video_id(url_input)
        with st.spinner(f'æ­£åœ¨æ·±åº¦æŠ“å–å½±ç‰‡ ID: {v_id} çš„å…¨éƒ¨ç•™è¨€...'):
            data = get_video_comments(v_id)
            if data:
                st.session_state.result_df = pd.DataFrame(data)
                st.session_state.result_video_id = v_id
        st.session_state.running = False
        st.rerun()

# --- çµæœé¡¯ç¤ºå€ ---
if st.session_state.result_df is not None:
    df = st.session_state.result_df
    v_id = st.session_state.result_video_id

    st.success(f"âœ… æŠ“å–å®Œæˆï¼ç¸½è¨ˆï¼ˆå«å›è¦†ï¼‰å…± {len(df)} ç­†è³‡æ–™ã€‚")
    st.dataframe(df, width='stretch')

    csv = df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
    st.download_button(
        label="ğŸ“¥ ä¸‹è¼‰å®Œæ•´ CSV æª”æ¡ˆ",
        data=csv,
        file_name=f"comments_{v_id}.csv",
        mime="text/csv",
    )

    st.divider()
    generate_wordcloud(df)
